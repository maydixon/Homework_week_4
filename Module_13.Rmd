---
title: "Module 13"
output:
    github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo=TRUE,
	warning=FALSE,
	comment="##",
	prompt=TRUE,
	tidy=TRUE,
	tidy.opts=list(width.cutoff=75),
	fig.path="img/"
)
```
# Elements of Regression Analysis

## Preliminaries
- Install this package in ***R***: {curl}

## Objectives
> The objective of this module is to continue our discussion of simple linear regression analysis to understand how regression partitions the variance in the response variable among different sources, that explanined by the regression model and the error or residual variance. We also discuss the concept of residuals and how to calculate a measure of uncertainty - the standard error - for our regression coefficients and for the predicted values of our response variable based on a regression model. We also briefly discuss ways to transform non-normally distributed data to make them more appropriate for analysis using linear regression.

## Analysis of Variance and ANOVA Tables

In our linear models, we can separate or "partition" the total variation in our **y** variable (the *sum of squares of y*, or SSY) into that explained by our model (the regression sum of squares, or SSR) and that which is left over as "error" (the error sum of squares, or SSE): $SSY$ = $SSR$ + $SSE$.

Graphically...

<img src="img/partitioningSS.png" width="500px"/>

Let's make sure we have our zombie data loaded...

``` {r}
library(curl)
f <- curl("https://raw.githubusercontent.com/difiore/ADA2016/master/zombies.csv")
d <- read.csv(f, header = TRUE, sep = ",", stringsAsFactors = FALSE)
```

Now, we run a straightforward bivariate regression model and, using the raw data (which are duplicated in the model object), we calculate the various sums of squares of our variables and identtify the numbers of degrees of freedom.

``` {r}
m <- lm(data=d,height~weight)
SSY <- sum((m$model$height-mean(m$model$height))^2) # height - mean(height)
SSY
SSR <- sum((m$fitted.values-mean(m$model$height))^2) # predicted height - mean height
SSR
SSE <- sum((m$model$height-m$fitted.values)^2) # height - predicted height
SSE
```
From here, we can calculate the variance in each of these components, typically referred to as the *mean square*, by dividing each sum of squares by its corresponding degrees of freedom (recall that a variance can be thought of as an average "sum of squares").

The degrees of freedom for the regression sum of squares (SSR) is equal to the number of predictor variables, in this case one (if we know the value of one predictor variable, then we know the predicted value of the response variable). The degrees of freedom for the error sum of squares (SSE) is equal to $n-2$; we need to estimate two parameters from our data ($\beta_0$ and $\beta_1$) from our data before we can calculate the error sum of squares. Finally, the number of degrees of freedom for the total sum of squares (SSY) is $n-1$... we need to estimate one parameter from our data (the mean value of **y**) before we can calculate SSY.

The last item in the table, the **F ratio**, is the ratio of the variance explained by the regression to the remaining, unexplained variance: MSR/MSE.

``` {r}
df_regression <- 1
df_error <- 998
df_y <- 999
MSR <- SSR/df_regression
MSE <- SSE/df_error
MSY <- SSY/df_y
```

These values form the main entries in the  **ANOVA Table** for our regression.

| Source          | Sum of Squares | Degrees of Freedom | Mean Squares (SS/df) | F Ratio |
|:----------------:|:--------------:|:------------------:|:------------------------:|:--------:|
| Regression      | SSR = 12864.82       | 1                  | MSR = 12864.82                 | 2254.139  |
| Error       | SSE = 5693.79        | $n-2$ = 998        | MSE = 5.7072                     |
| Total       | SSY = 18558.61       | $n-1$ = 999        | MSY = 18.57719                    |

We can test the overall significance of our regression model by evaluating the F ratio with an F test, which evaluates a ratio of two variances according to the F distribution, taking into account the number of degrees of freedom in each. The critical value is given by `qf(p,df1,df2)`, where **p** is 1-$\alpha$ and **df1** and **df2** are the degrees of freedom in the sources being compared (regression versus error).

``` {r}
fratio <- qf(p=0.95,df1=1,df2=998)
fratio
```

For our data, then, the value for the F ratio exceeds this critical value.

Alternatively, we can use...

``` {r}
1-pf(q=2254.139,df1=1,df2=998)
```
... and we find the p value associated with this high of an F ratio is infintessimally small.

Using the ***R*** function `summary.aov()` with our model as an argument will provide the same results as we calculated by hand above.

``` {r}
summary.aov(m)
```

Note that the this table also shows the coefficient of determination, or the "R-squared value", which we identified above as the fraction of the total variation explained by the mode. This is simply SSR/SSY, and the correlation coefficient, $\rho$, is simply the square root of this value.

``` {r}
r_squared <- SSR/SSY
r_squared
rho <- sqrt(r_squared)
rho
```

## Standard Errors of Regression Coefficients

We can calculate standard errors for each of the various components of our regression model, i.e., the slope and intercept, and each predicted value of **y**.

The standard error of the regression slope, $\beta_1$, is calculated as:

<img src="img/sebeta1.svg" width="175px"/>

``` {r}
SSX <- sum((m$model$weight-mean(m$model$weight))^2)
se_beta1 <- sqrt(MSE/SSX)
se_beta1
```

The standard error of the intercept, $\beta_0$, is calculated as:

<img src="img/sebeta0.svg" width="260px"/>

``` {r}
se_beta0 <- sqrt((MSE*sum(m$model$weight^2))/(1000*SSX))
se_beta0
```

Finally, the standard error of each predicted value of y is calculated as:

<img src="img/seyhat.svg" width="350px"/>

``` {r}
se_yhat <- sqrt(MSE*(1/1000+(m$model$weight-mean(m$model$weight))^2/SSX))
head(se_yhat) # just the first 6 rows
```

These same standard errors for $\beta_0$ and $\beta_1$ are exactly what are returned by the `lm()` function.

``` {r}
summary(m)
```

## Model Checking

So far, we've derived a bunch of summary statistics describing our model and we've looked at ways of testing whether those summary statistics are significantly different from zero. That is...

- We've seen whether our overall regression model explains a significant portion of the variance in **y** by means of the F ratio test
- We've calculated standard errors for our $\beta_1$ and $\beta_0$ estimates and seen whether they are significantly different from zero by means of t tests
- We've calculated standard errors for our prediction of **y** (<img src="img/yhat.svg" width="12px"/>) at each value of **x**
- We've estimated the proportion of the total variance in **y** explained by our model (i.e., "R squared")

What we haven't done yet, however, is checked our model fit critically in other ways... particularly, we haven't seen whether two assumptions of linear modeling are met: that our residuals are normally distributed and that there is constancy of variance in our **y** values across the range of **x**s.

We can investigate our residuals as one way of assessing model fit. 

#### CHALLENGE:

Calculate the residuals from the regression of zombie height on weight and plot these in relation to weight (the **x** variable). There are lots of ways to do this quickly.

``` {r}
m <- lm(data=d,height~weight)
plot(x=d$weight,y=m$residuals)
# or
e <- resid(m)
plot(x=d$weight,y=e)
```

Now, plot a histogram of your residuals... ideally they are normally distributed!

``` {r}
hist(e, xlim=c(-4*sd(e),4*sd(e)),breaks=20, main="Histogram of Residuals")
```

An additional way to quickly examine your residuals is to use the `plot()` function with your model as an argument. This prints out 4 plots that each tell you something.

``` {r}
plot(m$model$weight,m$residuals)
par(mfrow=c(2,2))
plot(m)
```
The first plot of fitted values (<img src="img/yhat.svg" width="12px"/>) versus residuals should, like the plot of **x** versus residuals, not show any structure. We hope to see equally spread residuals around a horizontal line without distinct patterns. The second plot is a Q-Q plot of theoretical quantiles versus standardized quantiles for the residual values. These should fall on roughly a straight line, if the residuals are normally distributed. The third plot graphs the square root of the standardized residuals versus **x** and shows whether or not residuals are spread equally along the ranges of **x**s. It is good if you see a horizontal line with equally spread points rather than a decrease or increase in spread with **x**, which would indicate that the error variance increases or decreases with **x**. The fourth plot highlights whether there are particular observations that influence the results. In particular, we look to see if there are cases that fall in the upper or lower right portion of the plot.

#### CHALLENGE:

Load in the "KamilarAndCooper.csv" dataset and develop a linear model to look at the relationship between "weaning age" and "female body mass". You will probably need to look at the data and variable names again to find the appropriate variables to examine.

- Using the procedures outlined above and in Module 12, calculate estimates of $\beta_0$ and $\beta_1$ by hand ***and** using the `lm()` function. Are the regression coefficients estimated under a simple linear model statistically significantly different from zero?
- Construct an ANOVA table by hand and compare your values to the results of running `lm()` and then looking at `summary.aov(lm())`.
- Generate the residuals for your linear model by hand, plot them in relation to female body weight, and make a histogram of the residuals. Do they appear to be normally distributed?
- Run the `plot()` command on the result of `lm()` and examine the 4 plots produced. Again, based on examination of the residuals, does it look like your model has good fit?

``` {r}
f <- curl("https://raw.githubusercontent.com/difiore/ADA2016/master/KamilarAndCooperData.csv")
d <- read.csv(f, header = TRUE, sep = ",", stringsAsFactors = FALSE)
head(d)
plot(data=d,WeaningAge_d~Body_mass_female_mean)
model<-lm(data=d,WeaningAge_d~Body_mass_female_mean)
summary(model)
plot(model)
```

## Data Transformations

Recall that two conditions that need to be true for linear regression modeling to be appropriate is that our variables should be **normally distributed** and that should be **homogeneity of variance** in our response variable around the range of our predictor variable.

In many cases, the continuous metric data we have are not, in fact, normally distributed. Nonetheless, we can often apply some kind of mathematical transformation to our data to change their distribution to more closely approximate the normal. 

The logarithmic or "log" transformation (where we take the log value of each data point) is often applied to positive numeric variables with heavy skew to dramatically reduce the overall range of the data and bring extreme observations closer to a measure of centrality. The logarithm for a number is the power to which you must raise a base value (e.g., $e$, for the natural log) in order to obtain that number. This is an example of a "power transformation", other examples of which include the square root transformation and the reciprocal (or multiplicative inverse) transformation.

#### CHALLENGE:

Return to the "Kamilar and Cooper"" dataset you were looking at above and log transform both of your variables and then run your linear model. Do you notice a difference?

``` {r}
d$logWeaningAge <- log(d$WeaningAge_d)
d$logFemaleBodyMass <- log(d$Body_mass_female_mean)
plot(data=d,logWeaningAge~logFemaleBodyMass)
model<-lm(data=d,logWeaningAge~logFemaleBodyMass)
summary(model)
plot(model)
```

The following chart shows some other common numerical transformations.

<img src="img/transformations.png" width="500px"/>